---
title: "Are Databricks and Delta Lake Right for You?"
date: 2023-08-01T11:48:04-07:00
draft: true
---

![DataBricks logo](/databricks_logo.png)

# The Impact of DataBricks' New Approach

In recent months I've been trying to get as much experience with DataBricks (and their Delta Lake storage framework which is the foundation of the lakehouse data architecture) as I can. There are a lot of tools out there in the data space, and a lot of companies claiming to offer products that will transform the way you and your company work with data, but not all of them live up to the hype. In almost twenty-five years of working with data I've seen a lot of products come and go. The real test is which ones make a lasting impact and truly change the way that organizations work with data (rather than simply catching up or capitalizing on the trends that others have created). 

## Why DataBricks is Receiving So Much Attention

DataBricks has generated a lot of headlines and discussion, but I think it's safe to say that this buzz is not hype. They've created an architectural model (the **[lakehouse](https://www.databricks.com/glossary/data-lakehouse)**), an open source storage framework (**[Delta Lake](https://delta.io)**), and a cloud data processing platform ([DataBricks](https://www.databricks.com) itself) that together represent a leap forward in working with data...

## Introduction To Key Terms

The rest of this article will discuss these key terms in more detail, but I want to give a brief overview to clarify the most important points so this isn't just an exercise in jargon:

- **Delta Lake** is an open source framework, created by the DataBricks team, for working with all kinds of data. It includes a storage layer over cloud object stores (like AWS S3 and Google Cloud Storage), 'a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets,' and 'high-level features such as automatic data layout optimization, upserts, caching, and audit logs.' Delta Lake tables can be accessed from many existing tools like Spark, Hive, Redshift, and others. This academic white paper gives a thorough theoretical and technical definition of Delta Lake and how it works: **[Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores](/delta-lake-paper.pdf)**. For more technical readers, I highly recommend that whitepaper as it will give you a thorough introduction to the delta lake framework, the issues that it addresses, and the details of the implementation. For even more info and to check out the code directly, see the **[Delta Lake GitHub repository](https://github.com/delta-io/delta)**.
- **lakehouse**. The term is meant as a mash-up of 'data warehouse' and 'data lake,' since the lakehouse combines important features of both: standard DBMS management functions (as in data warehouses) usable directly against low-cost object stores (as in data lakes).' But the lakehouse model, while incorporating important features of both warehouses and lakes, goes beyond either separate model. The Delta Lake framework allows lakehouses to incorporate features from both data warehouses and data lakes, and also to incorporate streaming data and to address many of the issues inherent to warehouse and lake architectures. More on this below. For a detailed overview of the lakehouse architecture, see this book:
**[The Data Lakehouse Platform for Dummies](/data_lakehouse_dummies.pdf)**. This book is the perfect place to start for understanding not only the lakehouse architecture, but the bigger picture of the state of affairs in the data space. It provides an excellent brief history of data management approaches; explains the concept of the lakehouse including key features (like support for both BI and ML, ability to handle structured and unstructured data, and ability to scale) and common problems it is designed to solve (such as handling data in real-time, performance and data quality issues, and lack of ACID transactions); gives detailed comparison with other DM (Data Management) approaches (the warehouse and lake); and explains the many ways in which lakehouses add significant value to DM operations (such as reliability, enabling Business Intelligence [BI] on all data and not only that stored in a structured way in the warehouse, and increasing data science productivity); and gives an overview of how to use Delta Lake and Delta Bricks to implement the lakehouse architecture.
- DataBricks

## Detailed Dive

### Delta Lake

Now let's discuss each of these concepts in more detail. First let's dive into the Delta Lake framework, because there's a lot to unpack there (see the [academic whitepaper](/delta-lake-paper.pdf) if you haven't already). Even if you're not familiar with a lot of the technical details of databases, anyone who's worked with data is familiar with the most common form of them, the *[relational database](https://en.wikipedia.org/wiki/Relational_database)* (Like MySQL or Microsoft SQL Server). Relational databases are still ubiquitous, despite being largely unchanged in form for a few decades, because they offer a number of essential features, such as structured access to data and *[ACID](https://en.wikipedia.org/wiki/ACID)* (Atomicity, Consistency, Isolation, Durability) transactionality... 
Object-based cloud storage formats are also ubiquitous because they too offer many great features: ...
Delta Lake combines the best of both worlds of relational databases and cloud storage, with additional features. It allows you to take advantage of low-cost, easily accessible cloud storage and use it like you would a database (with ACID transactionality), and also to unify stream and batch processing (which is a significant limitation of standard databases).
What's great about the Delta Lake framework is that is built on years of real-world experience in working with customers' data. DataBricks existed as a cloud service provider starting in 2014, and in the first few years of their existence they noticed that customers were running into a lot of the same problems when trying to use cloud-based data warehouse solutions - data corruption, consistency, and performance issues in particular. DataBricks was not the first company to try to take advantage of cloud storage as a basis for a data warehouse, and companies have experimented with various ways to implement such solutions. Delta Lake aims to address those common problems, and therefore it leverages the advantages of cloud storage while helping to avoid common pitfalls.
One advantage of Delta Lake is that it allows many companies to simplify

### Delta Lake Enables Lakehouse Architecture

Traditional data warehouses are still effective many use cases, but they have significant limitations. First, they are not suitable for semi-structured or unstructured data. Data warehouses typically follow the standard RDBMS model in which schema are strictly defined, meaning that they do not do a good job handling data without this predefined structure. Data lakes help solve this problem by introducing the ability to deal with data in its raw form, meaning that they are more suitable for working with less structured data. But data lakes have significant limitations because they do not support many data warehouse features like transactions, they do not enforce data quality, and their lack of consistency/isolation makes it almost impossible to mix appends and reads, and batch and streaming jobs. And on top of all this, both the data warehouse and data lake are limited in their ability to deal with streaming data, which is becoming increasingly common in application deployment and analytics. Tools like the Spark streaming toolkit can help to mitigate some of this limitation, but the point still stands that neither data warehouses and data lakes are built from the ground up to handle streaming data, so for serious streaming use cases another tool is usually necessary. The lakehouse represents the best of many worlds because it can effectively handle structured, semi, and unstructured data, and can also handle streaming data... Data teams often develop solutions that stitch together the two models (warehouse and lake), but these are not ideal and create friction between the two models, and often lead to wildly overcomplicated architectures in which many features are present to handle edge cases or expose isolated parts of the architecture. The lakehouse is a powerful solution for many companies because it incorporates key features of warehouses and lakes, but are a holistic solution that does not merely represent a stitching together of two separate models.

[complicated architecture diagram]
