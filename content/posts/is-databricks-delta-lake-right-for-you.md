---
title: "Are Databricks and Delta Lake Right for You?"
date: 2023-08-01T11:48:04-07:00
draft: true
author: "Guy Cutting"
tags: ["DataBricks", "Delta Lake", "Lakehouse", "Data Architecture", "Data warehouse", "Data Lake", "Databases", "ACID", "Transactions", "Business Intelligence", "Cloud Storage", "Data Processing", "Batch Data", "Streaming Data"]
categories: ["Data Architecture"]
---

![DataBricks logo](/databricks_logo.png)

# The Impact of DataBricks' New Approach

Over the last few months I've been trying to get as much experience with DataBricks (and their Delta Lake storage framework which is the foundation of the lakehouse data architecture) as I can. There are a lot of tools out there in the data space, and a lot of companies claiming to offer products that will transform the way you and your company work with data, but not all of them live up to the hype. In almost twenty-five years of working with data I've seen a lot of products come and go. The real test is which ones make a lasting impact and truly change the way that organizations work with data (rather than simply playing catch-up or capitalizing on the trends that others have created). 

One thing that impresses me about DataBricks is that they're a thought leader in the data space, and they take pains to provide a wealth of information to help companies make important decisions about how to make the most of their data. I recently made another post focusing on some of these resources that Databricks makes available, particularly whitepapers and ebooks covering Databricks itself and related products (like Spark and the Delta Lake framework):

[Databricks Resources for Practicioners, Managers, and Decision-makers](/databricks-resources.md)

## Why DataBricks is Receiving So Much Attention

DataBricks has generated a lot of headlines and discussion in recent years, but I think it's safe to say that this buzz is not hype. They've created an architectural model (the **[lakehouse](https://www.databricks.com/glossary/data-lakehouse)**), an open source storage framework (**[Delta Lake](https://delta.io)**), and a cloud data processing platform ([DataBricks](https://www.databricks.com) itself) that together represent a leap forward in working with data. I have been in the tech industry for twenty years, and for the last decade I've been focusing on data, first as a data analyst, then a data scientist, more recently as a data engineer, and finally now as a consultant working across all these areas. Identifying myself as a data practicionery is important because, fter all, data is a singular area of business concern, and though roles are important, they often serve to silo data operations into areas which don't interact well in the organization. I've been quite impressed with Databricks (the company and the software), because it is made up of people who truly understand the many challenges that companies face today in the world of data, and they are offering concepts, products, and services which address the biggest and most difficult problems in a holistic way, enabling companies to transform their data operations and cultures (rather than just addressing narrow technical concerns). There's a reason that [Gartner has named DataBricks a leader two years running](https://www.databricks.com/resources/analyst-paper/databricks-named-leader-by-gartner) in Cloud Database Management Systems: their products and services stand out from the pack, and they offer companies tremendous value that can't be found anywhere else in the industry.



## Introduction To Key Terms

The rest of this article will discuss these key terms in more detail, but I want to give a brief overview to clarify the most important points so this isn't just an exercise in jargon:

- **[Delta Lake](https://delta.io)** is an open source framework, created by the DataBricks team, for working with all kinds of data. It includes a storage layer over cloud object stores (like AWS S3 and Google Cloud Storage), 'a transaction log that is compacted into Apache Parquet format to provide ACID properties, time travel, and significantly faster metadata operations for large tabular datasets,' and 'high-level features such as automatic data layout optimization, upserts, caching, and audit logs.' Delta Lake tables can be accessed from many existing tools like Spark, Hive, Redshift, and others. This academic white paper gives a thorough theoretical and technical definition of Delta Lake and how it works: **[Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores](/delta-lake-paper.pdf)**. For more technical readers, I highly recommend that whitepaper as it will give you a thorough introduction to the delta lake framework, the issues that it addresses, and the details of the implementation. For even more info and to check out the code directly, see the **[Delta Lake GitHub repository](https://github.com/delta-io/delta)**.
- The **[lakehouse](https://www.databricks.com/glossary/data-lakehouse)** is a conceptual development, a relatively new data architecture which addresses many of the shortcomings of more long-established data management approaches. The term is meant as a mash-up of 'data warehouse' and 'data lake,' since the lakehouse combines important features of both: standard DBMS management functions (as in data warehouses) usable directly against low-cost object stores (as in data lakes).' But the lakehouse model, while incorporating important features of both warehouses and lakes, goes beyond either separate model. The Delta Lake framework allows lakehouses to incorporate features from both data warehouses and data lakes, and also to incorporate streaming data and to address many of the issues inherent to warehouse and lake architectures. More on this below. For a detailed overview of the lakehouse architecture, see this book:
**[The Data Lakehouse Platform for Dummies](/data_lakehouse_dummies.pdf)**. This book is the perfect place to start for understanding not only the lakehouse architecture, but the bigger picture of the state of affairs in the data space. It provides an excellent brief history of data management approaches; explains the concept of the lakehouse including key features (like support for both BI and ML, ability to handle structured and unstructured data, and ability to scale) and common problems it is designed to solve (such as handling data in real-time, performance and data quality issues, and lack of ACID transactions); gives detailed comparison with other DM (Data Management) approaches (the warehouse and lake); and explains the many ways in which lakehouses add significant value to DM operations (such as reliability, enabling Business Intelligence [BI] on all data and not only that stored in a structured way in the warehouse, and increasing data science productivity); and gives an overview of how to use Delta Lake and Delta Bricks to implement the lakehouse architecture.
- DataBricks

## Detailed Dive

### <a name = "delta-lake-architecture">Delta Lake</a>

Now let's discuss each of these concepts in more detail. First let's dive into the Delta Lake framework, because there's a lot to unpack there (see the [academic whitepaper](/delta-lake-paper.pdf) if you haven't already). Even if you're not familiar with a lot of the technical details of databases, anyone who's worked with data is familiar with the most common form of them, the *[relational database](https://en.wikipedia.org/wiki/Relational_database)* (Like MySQL or Microsoft SQL Server). Relational databases are still ubiquitous, despite being largely unchanged in form for a few decades, because they offer a number of essential features, such as structured access to data and *[ACID](https://en.wikipedia.org/wiki/ACID)* (Atomicity, Consistency, Isolation, Durability) transactionality... 

For many applications, relational databases are still essential, despite the rise of [NoSQL] and other storage systems. Take the e-commerce example, where you have structured information about customers, orders, order items, payments, and other entities, with frequent small transactions (adding a customer account, placing an order, updating payment information). These data entities are well-defined (*structured data*), and the relationships between them are well understood at the outset (for example, every order has an associated customer, one or more items, and a payment method). For such an application, relational databases are, and always will be, essential, because they provide key features like data schema and ACID transactionality (which ensures data validity even when errors happen).

<a name = "rdbms"></a>
#### The Evolution of Data Management - RDBMS (OLTP)

RDBMS are also known as OLTP (OnLine Transaction Processing) systems. They store data by rows, because this is how transactions are usually grouped. When a new customer account is created, for example, a single transaction inserts all related customer information (name, address, phone number, etc.) into the system in a single transaction. OLTP systems, while they are optimized for these sorts of transactional applications, are not optimized for analytical workloads. OLAP (OnLine Analytical Processing) systems (like BigQuery or Redshift), by contrast, are optimized for analytics applications in which processing usually focuses on calculations involving many different records (like calculating the average sale amount for a group of orders). OLAP systems, as a result, are often referred to as *columnar* databases, because they store data with values grouped together by column (instead of by row as in OLTP systems). The term OLAP is generally interchangable with 'data warehouse'. Data warehouse systems are typically column-storage databases which are optimized for analytical workloads, which involve frequent aggregation and grouping in order to calculate values of interest. An online retailer might want to calculate the average size of order by month and customer type (business or invidual), so an analytical query would group by month and customer type and aggregate order amount across these groups. 

#### The Evolution of Data Management - OLAP

RDBMS are unsuitable for analytics, since they are optimized for individual transactions and not for aggregate calculations across many rows. Despite this unsuitability, many companies still attempt, naively, to use them in this way. A couple of years ago I worked with a company that had built a proprietary data warehouse in MySQL back in about 2016. That was a bad decision, even then, since already by 2016 many OLAP systems were commercially available... This mistake (sometimes called *abusing OLTP as OLAP*) is a common one. RDBMS are ubiquitous and just about every organization relies on many of them, since they are used by in many different software products. This company had its own proprietary web application product (a jobs website) which stored data in MySQL. Despite the availability of dedicated OLAP systems, they decided to build their own data warehouse in MySQL, because their data was already present in MySQL, they didn't want to deal with setting up and maintaining an additional database system, and they weren't sure how they would keep data synchronized between the two systems.

#### Poor Architectural Choices - Case Study

These sorts of problems (like syncing data between database systems) are well-understood (at least by some practicioners) and there are many tools available to solve them. But this company, in part because it occupied a market niche and had never faced much competitive pressure, simply had never had much incentive to bring their analytics culture into the twenty-first century. They were still handling data like companies used to in the early 2000s, emailing around spreadsheets and compiling numbers essentially by hand. I helped them migrate their data to a Redshift-based data warehouse, which enabled them to introduce automated reporting (using Tableau and other tools), dramatically reducing the time spent on compiling business reports. That sort of migration (to a dedicated data warehouse) was leading edge in the early 2000s, when products like Redshift became commercially available. But because of poor data-related decision making and a lack of available expertise, this company was struggling with problems that have been solved many times in other places, and which dramatically impact their business operations.

The company I'm referring to had a badly broken data culture. They didn't realize the value of their data, and didn't realize the value of dedicated data analytics professionals (as opposed to developers). Their previous data warehouse was so poorly implemented, and their data culture was so badly broken, that they were making decisions based on wrong or incomplete data, and often significantly misunderstood their own operations as a result. It was only because they occupied a unique market niche that this company was even able to stay in existence and remain somewhat profitable. But if a competitor came along and built a similar tool to connect job seekers and employers in the same industry, they could easily be displaced in a matter of months, because they did not have a good picture of their own business operations (like the number and frequency of jobs posted, and the number and types of job seekers in various categories). These sound like mistakes that any company should know enough to avoid, but they are mistakes that companies make every day, and they arise from a poor data culture in which the true value of data is not appreciated and not enough investment (in money and time) is being made to enable decision makers to have quick access to accurate and meaningful data.

#### The Evolution of Data Management - Object Storage and the Benefits of Delta Lake

Object-based cloud storage formats are also ubiquitous because they too offer many great features: ...
Delta Lake combines the best of both worlds of relational databases and cloud storage, with additional features. It allows you to take advantage of low-cost, easily accessible cloud storage and use it like you would a database (with ACID transactionality), and also to unify stream and batch processing (which is a significant limitation of standard databases).
What's great about the Delta Lake framework is that is built on years of real-world experience in working with customers' data. DataBricks existed as a cloud service provider starting in 2014, and in the first few years of their existence they noticed that customers were running into a lot of the same problems when trying to use cloud-based data warehouse solutions - data corruption, consistency, and performance issues in particular. DataBricks was not the first company to try to take advantage of cloud storage as a basis for a data warehouse, and companies have experimented with various ways to implement such solutions. Delta Lake aims to address those common problems, and therefore it leverages the advantages of cloud storage while helping to avoid common pitfalls.
One advantage of Delta Lake is that it allows many companies to simplify

### Delta Lake Enables Lakehouse Architecture

Traditional data warehouses are still effective many use cases, but they have significant limitations. First, they are not suitable for semi-structured or unstructured data. Data warehouses typically follow the standard RDBMS model in which schema are strictly defined, meaning that they do not do a good job handling data without this predefined structure. Data lakes help solve this problem by introducing the ability to deal with data in its raw form, meaning that they are more suitable for working with less structured data. But data lakes have significant limitations because they do not support many data warehouse features like transactions, they do not enforce data quality, and their lack of consistency/isolation makes it almost impossible to mix appends and reads, and batch and streaming jobs. And on top of all this, both the data warehouse and data lake are limited in their ability to deal with streaming data, which is becoming increasingly common in application deployment and analytics. Tools like the Spark streaming toolkit can help to mitigate some of this limitation, but the point still stands that neither data warehouses and data lakes are built from the ground up to handle streaming data, so for serious streaming use cases another tool is usually necessary. The lakehouse represents the best of many worlds because it can effectively handle structured, semi, and unstructured data, and can also handle streaming data... Data teams often develop solutions that stitch together the two models (warehouse and lake), but these are not ideal and create friction between the two models, and often lead to wildly overcomplicated architectures in which many features are present to handle edge cases or expose isolated parts of the architecture. The lakehouse is a powerful solution for many companies because it incorporates key features of warehouses and lakes, but are a holistic solution that does not merely represent a stitching together of two separate models.

[complicated architecture diagram]
