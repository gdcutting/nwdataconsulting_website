[{"categories":null,"content":"I’m thrilled to find an ideal set up for this site, which uses GitHub Pages for hosting and GitHub Actions for automated build and deployment. There was one last piece to the puzzle of finally launching the site, and that was configuring the custom DNS with GitHub Pages. When Pages hosts your site, it does so by default at a URL like https://gdcutting.github.io/nwdataconsulting/. There’s nothing wrong with that URL as-is, but for branding a business, we obviously want a top-level domain that reflects the business name (like https://www.nwdataconsulting.com). So how do we set up a custom domain with a GitHub Pages site? The GitHub Docs are a good place to start (but I’m going to provide a couple of key points of clarification): Custom Domains and GitHub Pages That page is a good place to start, and make sure to click through the following docs pages (on managing, verifying, and troubleshooting custom domains). These docs provide everything you need to know to get your custom domain set up, but a key point (the difference between apex domains and subdomains) gets lost in all the verbiage. The very first thing to do to set up a custom domain is simply to set your custom domain under the ‘Custom domain’ setting in the Pages section of your GitHub repository settings: One of the things that the automated deployment option I’m using simplifies is further DNS setup for a custom domain. If you’re doing your own build (i.e. not using GitHub Actions for deployment), you need to add a CNAME record to your repository, but since we’re using GitHub Actions, that’s not necessary - GitHub handles everything on its end after you enter you custom domain in the Pages settings. The final bits of configuration needed for your custom domain are a couple of DNS record additions. I’m using DreamHost so I’ll walk through the process with their DNS management; it should be substantially similar with any other domain name provider. The part I want to make clear here is how to handle BOTH the apex domain (nwdataconsulting.com) and the www subdomain (www.nwdataconsulting.com). When we’re being casual about it we don’t distinguish between the two - but the DNS machinery does in fact distinguish between them, and we need DNS record changes to BOTH of these for the site to be fully accessible. What you’ll need for this is: an ALIAS record for your apex domain (without the www) a CNAME record for your www. subdomain These records will both point to the same place (gdcutting.github.io for me, or whatever your appropriate URL is for your GitHub username), but they have to be set separately. Here’s how to set the ALIAS record: And here’s how to set the CNAME record: Notice that the ALIAS record doesn’t use the www. subdomain and the CNAME record does (that’s the important difference). GitHub Pages does handle the basic redirect for you (i.e. if you have both apex and subdomains configured in your DNS, Pages will automatically redirect domain.com to www.domain.com). But you still need both DNS records for the site to be fully accessible (otherwise you’ll get a network error page on domain.com). Notice in the above pic of Pages settings that GitHub automatically performs a DNS check on both apex and www subdomains to see if they are configured correctly, and it will give you a big yellow or red warning or error message to let you know if one are both of them aren’t configured right. I was thrown off by the redirect part, because I thought that might be handled on the DreamHost side. It turns out (as the GitHub docs do state, ahem) that GitHub handles this redirect (from nwdataconsulting.com to www.nwdataconsulting.com) for you, but only if you have BOTH the ALIAS and CNAME records set up. You can also set up additional subdomains (like blog.nwdataconsulting.com). I’ll skip those details here because the GitHub docs linked above do a good job with that. Another thing to note (and another perk of using GitHub Pages), is that you get a secure certicate for your site ","date":"2023-07-23","objectID":"/posts/github-pages-dns-configuration/:0:0","tags":null,"title":"Github Pages Custom Domain DNS Configuration","uri":"/posts/github-pages-dns-configuration/"},{"categories":null,"content":"ChatGPT Model Performance In Question OpenAI has been making all sorts of headlines in recent months after its breakthrough performance with the Chat-GPT3 model. One of those records was the fastest application debut (in terms of user downloads) in history, reaching 100m users in January, just two months after launching. But serious questions are already emerging about the long-term trajectory of the company and its flagship chat models. User data from May and June showed a ten percent drop in user base from month to month. There is speculation that this drop is due to the end of the spring college term, and speaks to the fact that a significant portion of the ChatGPT user base is college students using the tool for research (and to cheat, though some argue that teachers should use ChatGPT as a tool to work with rather than against). Related to this drop in user base is another, perhaps more concerning, set of question related to the performance of the newer versions of the ChatGPT models. In the last couple of weeks there has been a lot of good reporting which appears to confirm a phonemenon that users had already been reporting: that the performance of the newer versions of the model is actually worse than the previous versions. ","date":"2023-07-23","objectID":"/posts/openai-model-performance-degrading/:1:0","tags":null,"title":"OpenAI Model Performance Degrading In Recent Weeks","uri":"/posts/openai-model-performance-degrading/"},{"categories":null,"content":"New Research Paper Quantifies Performance Changes There has been a lot of reporting on this subject, such as: ChatGPT’s Performance Is Slipping, New Study Says I’m just linking that one piece since the blitz of articles on the topic mostly all make the same points, and are based heavily on this study by Stanford and Berkeley researchers, published July 18 on arXiv: How is ChatGPT’s Behavior Changing Over Time? The abstract from that paper is as follows: GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the “same” LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLM quality. I’ll let the article and the paper give you the details, but this paper and the discussion around it definitely raise some important questions. ","date":"2023-07-23","objectID":"/posts/openai-model-performance-degrading/:1:1","tags":null,"title":"OpenAI Model Performance Degrading In Recent Weeks","uri":"/posts/openai-model-performance-degrading/"},{"categories":null,"content":"What Is Happening Here? As someone with a strong background in data science (my Master’s degree is in Systems Science), I’m fascinated by this reporting, because I’m always curious to know what’s going on the under the hood of these models, and this recent reporting and analysis gives us some important information that can let us make inferences about some of those important but opaque technical changes. ","date":"2023-07-23","objectID":"/posts/openai-model-performance-degrading/:2:0","tags":null,"title":"OpenAI Model Performance Degrading In Recent Weeks","uri":"/posts/openai-model-performance-degrading/"},{"categories":null,"content":"More Parameters Is Not Always Better From a technical standpoint, one of the clear takeaways for me is that bigger is not always better. When I talk about size here, I’m thinking primarily of largeness as measured in the number of model parameters (i.e. the first ‘L’ in ‘Large Language Model’). When these LLMs first began to appear… Much of the recent discussion of LLMs has focused on Generative Pretrained Transformer (GPT) models, a subset of LLMs which emerged on the scene in around 2018 and have made spectacular progress since then. When these GPT models first began to appear, it seemed that there was a strong positive correlation The piece A Brief History of The Generative Pre-trained Transformer (GPT) Language Models gives a nice concise history of these OpenAI models in particular. GPT-1 (2018) contained 117 million parameters and was groundbreaking in being able to generate coherent sentences and paragraphs that were often indistinguishable from those generated by humans. GPT-2 (2019) contained 1.5 billion parameters and could generate even longer and more coherent text. The model’s creators reportedly withheld its full version because they were concerned that it could be used to generate fake news and other types of misleading content (although we should be wary here, too, since these sorts of claims could be their own devious form of marketing). GPT3 (2020) contained 175 billion parameters, was trained on an even larger corpus of data, can generate output indistinguishable from that produced by humans, and was the basis for the ChatGPT3 application that exploded so spectacularly on the scene last year. GPT-3.5 continued to refine model performance and led to even more eye-popping capabilities in the model’s ability to write code and perform other complex reasoning tasks. ChatGPT-4 continues to build on this series of models and includes the ability to process images as well, but OpenAI has declined to release technical details like the number of parameters. So it seems clear that, up to a certain point, increase the number of parameters in these models led to significant increases in performance. But it also becoming clear that, at this point in the model evolution, further increases in the size of the parameter space are unlikely to lead to performance improvements, and may even degrade performance. This is almost surely an LLM version of the overfitting problem, which is one of the first things anyone learns about when they begin to study data science. Overfitting is “the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably.” Given n data points, it is always possible to find an n-parameter model that will exactly fit the data (any two points can be fit by a line, any three points can be fit by a quadratic, any four points can be fit by a cubic, etc.). But simply adding more parameters is often a poor strategy because if it is followed blindly this strategy will generate models that, while they fit the underlying data well, have very poor performance in generalizing to data outside the training set… On an interesting side note, this line of analysis might speak to the lack of news of much-hyped VERY large LLM models (with numbers of parameters in the trillions) coming out of China in the last year or two. Perhaps this speaks to the idea that, once the fundamental limits of a given model architecture come into play adding more parameters alone will not increase performance. We have heard a litany of claims about performance breakthroughs from some of these Chinese models, but no publicly available application has grabbed headlines like ChatGPT did, which maybe just means that the ChatGPT-3 level of performance on language generation tasks was already so good that it’s hard to raise the bar much purely in that realm. The advances with Wu Dao and these newer Chinese models seems to be in th","date":"2023-07-23","objectID":"/posts/openai-model-performance-degrading/:2:1","tags":null,"title":"OpenAI Model Performance Degrading In Recent Weeks","uri":"/posts/openai-model-performance-degrading/"},{"categories":null,"content":"Complexity Is A Huge Factor One of the main themes of systems science is the study of complexity. When studying complexity it would be difficult to find a richer subject area than AI/ML. These systems are immensely complex and and of themselves, with LLM models representing highly evolved model architectures with now billions of parameters, and these models are trained and run on enormous distributed computing systems utilizing large numbers of GPUs optimized for throughput in the type of numerical calculation which form the basis of the AI workloads. But our understanding of AI as a system does not end there - these AI models and applications are but one part of an even more complex technical-social system in which human beings use the AI models to analyze and make decisions, and there are multiple interfaces between the human/social and technical systems. As the models grow larger and more complex, and their adoption becomes more widespread, the level of complexity has quickly outpaced our ability to analyze and understand it in the kind of depth required to effectively manage these systems. This phenomenon of changing LLM model performance is a great case in point. The technical-human system is evolving quickly as the human part of the system analyzes the performance of the technical part of the system and implements changes, and the changing performance of the technical system then requires updated analysis and reconfiguration in a constant feedback loop… This process is happening very quickly and with very little outside visibility from governments or other regulatory bodies that can process these changes and ensure that they are taking place with a framework of common values. Some people assume that security will be baked into the system, since market forces will act on OpenAI and other companies to force them to develop their products with security and safety in mind (since no company wants to be responsible for data breaches or, you know, the end of the world as we know it). But it is clear to me that the market forces are much more on the side of rapid profitability, especially at a time of significant anxiety about the economic climate. In the last year or so, stock markets have been in and out of bear status, and many analysts are openly looking to AI to turn the tide of what would otherwise be very poorly-performing markets. This CNN article from June 12 points out that, “Last Thursday, the S\u0026P 500 entered a bull market — up 20% from its recent lows. In a note on Friday, Bank of America economists said the move upwards was mostly because investors have bought into a singular equity theme: AI.” If investors see AI as a hedge against equities markets that, in the face of rapidly rising central bank interest rates, would otherwise be in the doldrums, then they are probably willing to overlook AI safety issues in order that AI companies can continue to produce a stream of headline-producing products that bouy a handful of high-performing tech stocks. We’ve seen this sort of thing before, with derivatives and subprime mortgages. There were some outliers who called out the fragility of the system, and predicted market mayhem when these financial engineering products began to fail. But the lure of easy profits for banks and mortgage companies was simply too much to resist, and calls to better regulate the industry were overlooked and even ridiculed until it was too late. By 2008, when Bear Sterns and then Lehman Brothers failed, it was too late to regulate subprime mortgages and the derivatives products which spread the rot across the entire financial system; by then the only thing governments could do was to try to halt further slide and bail out the banks. I worry that we are headed for something similar with the rapid adoption of poorly-understood AI products… One thing that LLM systems like ChatGPT have made clear is that in large part we fundamentally not understand how these systems work. Obviously we (or at least s","date":"2023-07-23","objectID":"/posts/openai-model-performance-degrading/:2:2","tags":null,"title":"OpenAI Model Performance Degrading In Recent Weeks","uri":"/posts/openai-model-performance-degrading/"},{"categories":null,"content":"Security Is A Major Performance Metric Oftentimes we distinguish between performance (how well does an application perform a given task) and security (whether the application can perform without vulnerabilities). But it is apparent in the realm of these newer AI models that security is (or should be) a major performance metric. The best-performing model does your business no good if you don’t feel safe using it. An ongoing conversation with a developer friend of mine is case in point for this issue in the fast-evolving realm of AI. The My friend (we’ll call him Ted) is a senior Java developer for a Portland tech company. He’s an early adopter in a lot of ways, and was one of the first people I knew to start using new tools like ChatGPT and Midjourney as they have become publicly available in recent months. At first, he was very impressed with the productivity boost he got from ChatGPT. He found it to be quite helpful in solving coding problems. While his experience was that ChatGPTs code suggestions often didn’t do exactly what he needed them to as generated, they got him far along on his solution to the problem, saving him a lot of the grunt work of code writing, which consists of solving multiple sub-problems along the way. For a few months, he used ChatGPT on a daily basis to help with his code work, and was very impressed with the results. But then articles like this one started to appear: OpenAI Says A Bug Leaked Sensitive ChatGPT User Data Ted’s company was one of many which, in the face of these reports of problems with data security with ChatGPT and other models, decided to ban its use on work computers. They decided that significant productivity increases were not worth the possibility of disclosure of their sensitive data. Ted continues to use ChatGPT on a more limited basis, but he has to do it on his personal computer and be very careful about what information he gives it. A lot of companies have found themselves in this conundrum, eager to take advantage of all the benefits that GPT models have to offer, but terrified that using them could expose sensitive data and create serious liability and other issues for them. We’re in a strange limbo where a lot of companies are waiting to see how these security issues play out before making the full plunge to using the public tools, and yet are afraid of being left behind as they watch many other business adopt their use. ","date":"2023-07-23","objectID":"/posts/openai-model-performance-degrading/:2:3","tags":null,"title":"OpenAI Model Performance Degrading In Recent Weeks","uri":"/posts/openai-model-performance-degrading/"},{"categories":null,"content":"What Are The Takeaways For Decision-Makers What is the way forward? The answer is not yet clear. Each enterprise has to make their own difficult decisions about the type of data they deal with and the degree of risk they are willing to tolerate. This conundrum is a big motivator behind recent calls for a moratorium on AI development and [ ]. The recent reporting on ChatGPT performance over time speaks to OpenAI’s attempts to address security issues. Part of the reason for some of the loss in performance that has been witnessed with these models is that OpenAI and other companies are intentionally limiting performance in order to reduce the likely of the model providing unsafe answers (i.e. there is some inherent tradeoff between performance and security). Here too complexity enters the picture. OpenAI can say that subsequent versions of the model and its applications (now including Bing Search) have fixed the bugs that caused unwanted data leakage, but how can we really be sure? The ChatGPT models have billions of parameters, but the complexity goes beyond just this measure. There is the model itself, but increasingly the application development has focused on filters to make output safe. OpenAI has in recent years de-prioritized the ‘open’ part of their name and mission in favor of rapid development, so we have to take them at their word on issues like safety. I support calls to make LLM-based applications, and AI in general, more open. Regulation is woefully inadequate (particularly in the US), and so far the industry has development outside of any meaningful regulatory regime (despite the understanding, at a high level, of all the significant security and safety issues that it poses.) I think that, as a society, we should legally enforce some degree of openness and disclosure about the models underlying AI applications so that lawmakers and regulators can make informed decisions about how they are allowed to be used. Regulation of AI is decades behind the development of AI itself, and there does not seem to be much political will to regulate it - unfortunately some major AI-related disaster (infrastructure vulnerability, stock market meltdown, etc.) might be what it takes to force attention on this issue. But again, that vague regulatory picture does little to help the people who are making decisions about AI adoption now, every day, out there in the real world. The technical background is fascinating, but for decision-makers (managers, executives, owners) the technical details pale in comparison to the questions of business logic, such as: What are the best models (and applications) available? What are the most important performance metrics for my particular business? How can I best integrate these newer applications (like GPT models) into my existing data stack and organization? If the performance of these models is constantly changing, how can I best choose where to focus long-term? w (call for pause)(call for more transparency) ","date":"2023-07-23","objectID":"/posts/openai-model-performance-degrading/:3:0","tags":null,"title":"OpenAI Model Performance Degrading In Recent Weeks","uri":"/posts/openai-model-performance-degrading/"},{"categories":null,"content":"Overview After experimenting with various ways to approach the site build, I finally find what I think is the best, most streamlined method: deploying on GitHub pages, using the automatic GitHub Actions to perform the Hugo build on the server side whenever changes are pushed to the build branch. This means that all I have to do to update the live site is to execute a handful of git commands (add files, commit, push), and everything else (the Hugo build and deploying the new rendered HTML and other files) happens automatically, making the process about as easy as possible. This avoids having to manually upload newly rendered files as I would have to do with another hosting provider, or if I was using GitHub pages but not using the automatic deployment option. The rest of this article gives details on how to get up the automated deployment. ","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:0:0","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"Choosing a theme with Hugo I finally got this site into shape with the theme (LoveIt) chosen and configured. One of the few issues that I have with Hugo is the lack of theme standardization. There’s not a lot of standardization enforced on theme developers, so there’s a very wide range of theme setups and configurations out there. As a result, theme testing can be onerous when you’re working on a new site. One of the really nice things about WordPress is that (since theme standardization is heavily enforced), changing themes is as easily as adding and activating the theme in the admin interface (which just requires a few clicks). With Hugo, you have to download the theme manually, decide whether or not to add the new theme as a Hugo module or git submodule, then figure out where in the theme configuration the options that are most important to you live. Sometimes the theme doesn’t work right out of the box, and you get a broken site and have to dig around to figure out the configuration change that will get you a properly working base configuration before you can even check out how the theme looks with your content and start to play with the options. This is by far the thing that I like least about the development cycle with Hugo, and unfortunately there’s no way around it, because the lack of theme standardization is at this point baked into how the project is run. ","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:1:0","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"Deployment Options The Hugo project has a page describing the process, from start to finish, of using automated deployment to perform your Hugo build with GitHub Pages: Host on Github Pages Note that you can simplify the above process a bit by choosing to let GitHub automatically supply a workflow file at step 4; this will eliminate step 5 (which has you create a blank file and past in content from the Hugo recommended workflow file - the GitHub default one works just as well). Once you choose a theme (which usually requires several rounds of the process described above, since for each candidate theme for your new site you at least have to do some downloading and basic setup, and usually on top of that you have to do some troubleshooting for a broken initial setup), the development cycle becomes a lot easier. At that point, when you’re adding your own content and working with the templating engine, you’re in more clearly mapped territory where there’s less room for confusion. Creating content obviously depends on you and not a theme developer, and the templating is thoroughly documented within the Hugo project (unlike themes). So now I’m at the point where I’m past the worst of the hassle and can just focus on the fun part, which is making content. But there’s one more issue to solve, and its an important one to get right since it will determine whether the site is easy to maintain and update in the future. That issue is how to deploy the site. The least technical way probably is to run Hugo locally every time there are changes, then just upload the newly rendered .html files to a web server. I have a Dreamhost account, and I could use it for this deployment, but I was interested in using GitHub Pages, partly to learn more about it and partly because my Dreamhost plan only includes one hosted site, and I can get the site hosted for free with Pages. With Pages, there’s more than one way to handle the deployment. From what I can tell, there are three in fact: as described above, run Hugo locally to render content additions/updates, and then manually push the files to GitHub (either using CLI git or through the GitHub web interface.) This method works fine but doesn’t take advantage of all the GitHub features that are available. the second method (see pick below) is to use GitHub’s automated deployment feature to deploy a static site. With this option, I still have to build the site locally (by running Hugo to render the new content and config changes). I’m not honestly sure why you’d use this option, since if you’re doing the Hugo build locally, the only thing left to do is to transfer the files to GitHub servers, which will happen automatically when you do whatever git workflow (CLI, through a VS Code extension, etc.) you do. GitHub Pages serves out of your repo, so once the files get updated there’s nothing else to update. If someone wants to fill me in on why you’d want to use this method, feel free. Maybe I’m missing something. the third and pretty clearly best method is to use GitHub automated deployment option with the remote Hugo build. With this method, once you push your files to the repo, GitHub performs the Hugo build automatically and your site gets updated with the new content. This makes it super easy to do content changes. My standard git workflow (I find myself still using CLI most of the time because it is faster) looks like: git add -A git commit -m 'my commit message' git pull git push which stages changes, performs the commit, pulls down changes from the remote repo, and pushes local changes to the remote. This just takes a few seconds, and once you do this, everything else (the Hugo build and site update) happens (mostly) behind the scenes on the GitHub side, using GitHub Actions. There’s a further bit of automation that we can employ here as well. Following this helpful article: Alias for git add, commit, and push all together I did, well, just what the title suggests, creating an alias to pack the most com","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:2:0","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"Choosing Between Github Deployment Options Here’s a shot of the repo settings GitHub Pages section showing the second two deployment options discussed above (under ‘Use a suggested workflow…’): ","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:2:1","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"How GitHub handles the deployment: Code-as-infrastructure There’s a bit of infrastructure-as-code that GitHub uses to handle these automated deployments - a .yaml file that lives in your directory and gives build parameters such as Hugo version, OS version, which branch to build from, etc: Most of this is boilerplate and you don’t need to worry about making changes there; it looks like there’s a lot to the workflow file but that .yaml workflow file is generated automatically when you select your preferred deployment action (Hugo build or statis site) under the Pages settings, so you don’t need to be familiar with all the details. But it is interesting to look under the hood to glean some info about how these builds are performed. The only thing you’ll probably want to check is the build branch, because if you’re not using main then you’ll want to specify that near the top with the branches parameter. ","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:3:0","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"Deployment In Action: Monitoring Progress Now that we’ve got the automation workflow set up, what happens when we run it, which happens automatically when any changes are pushed to the build branch (in my case my-pages)? One of the great things about GitHub is that it has all sorts of fancy machinery (hooks on the backend and JS on the front) to detect changes and display status in real time. When you trigger the build by pushing changes to the build branch, GitHub detects the changes, invokes your specified Action(s), and updates the status display in real time: ","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:4:0","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"Detailed Deployment Status This build progress page shows you an overview with the action number (22), the commit that triggered it (2adfda8), the ‘In Progress’ status, and that the Hugo build is ongoing, to be followed by the deployment. If you click on ‘deploy’, you’ll get a more detailed progress page for the deploy action: . We can assume that behind the scenes GitHub is spinning up a Docker container with a Linux machine to perform the deployment. If we clicked on ‘Operating System’ and ‘Runner image’ we would get more details here. You get some detailed deployment actions and confirmation that the deployment has been completed. Once you have your workflow set up, tested, and working correctly, it shouldn’t be necessary to look at these deployment action details very often (or ever). But if you do run into deployment issues, this progress page is where you can go for details that will help you troubleshoot whatever is going on. ","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:4:1","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"Using Commit History For Troubleshooting There’s another aspect of this automated deployment setup that I really like, and that’s keeping a clear record of the update/commit history to the site. This can be very useful when performing troubleshooting. Usually when you make a site update (particularly involving site or theme configuration), it is immediately clear if this change breaks something. If you make an errant configuration change, this typically manifests itself as a broken logo image or some other such obvious error in the rendered site. But that’s not always the case. Sometimes you make an update, and you don’t realize until later that, somewhere along the line, something got broken (particularly with site and theme configuration changes, which can interact in unexpected ways). And when you run into these issues where no immediate solution presents itself, troubleshooting can be very frustrating and time-consuming. Ever spend half a day tracking down a problem that just came down to one small change in one configuration file that you made hours ago and then forgot about? A change whose actual fix, once identified, takes literally just seconds, but anyone doing any serious web development work has experienced this incredibly frustrating situation at some point, where it takes hours to identify a problem that can be fixed with a few keystrokes. Automated deployment with Pages / Actions, in combination with descriptive commit messages, provides a powerful tool for tracking down errors in the site build. Let’s say I’m working on the site, clicking through pages, and I realize that something is awry. Usually in this case you think of the most recent commit as the culprit, but what if the most recent change could not have been responsible for the problem you’re seeing? In that case what do you do? Without a clear record of what was changed and when, you have to go through your own mental list of recent changes to isolate the culprit. If you aren’t using automated deployment with Actions (or some other tool), you don’t really have an easy way to do this. Server logs will tell you when a file was updated, but won’t show you the content of the update, so you’re purely reliant on your own memory and clicking around to look at all the possible sources of the problem. If you’re in a phase of making frequent content updates, this can be difficult. What if you’ve made twenty commits in the last two days? No one can remember the details of all those changes. And that’s just for a lowly single-proprietor website - what if you have a more complex site that’s being developed by multiple people simultaneously? The commit history stored by GitHub Actions (which we are using for automated site build and deployment) can be a powerful tool for simplifying this sort of troubleshooting. This is one reason why it’s important to follow the rule of commit frequently, push often (or I might phrase it as perform a push with every commit). That way, in the context of a GitHub Pages deployment with Hugo, you have exactly one commit for every automated build/deploy of the site. Combine that with using meaningful commit messages (i.e. ‘added logo image to footer partial’ rather than ‘updated footer’ or no commit message at all). If you follow this set of practices, your GitHub Actions log can quickly provide a lot of information about what was updated where and when: This is one of the oustanding things about GitHub and why so many developers use it as an integral part of their process. Look at how much information you can see in just that one page: a compact list of all your workflow runs with the newest first for each run, brief deployment details, a commit number, deployment branch, how long ago the deployment was performed, and how long the deployment took to complete from there, you can click into a branch or a commit to see further details about the Let’s see I’m interested in the last deployment action in the list in the above image (‘replaced","date":"2023-07-19","objectID":"/posts/automated-deployment-githubpages-hugo/:4:2","tags":null,"title":"Automated Hugo Deployment using GitHub Pages","uri":"/posts/automated-deployment-githubpages-hugo/"},{"categories":null,"content":"I recently came across a couple of articles that raise a common issue that organizations must address in dealing with their data: being overwhelmed. Here’s the first piece: How To Move Forward When You’re Feeling Overwhelmed By Data Another piece focuses on marketing data, but is relevant to the larger data discussion: 67% Of CMOs Say They Are Overwhelmed With Data Being overwhelmed is one of the biggest problems that organizations encounter when dealing with data. More data means more possibility of insight into the workings of your business, but more data also means more infrastructure and work to manage it. A lot of managers and executives think that if their company has a lot of reports and dashboards, they must be getting meaningful insight into their data. Too often, this is not the case. One of the best solutions to the problem of overwhelm is, unfortunately, not a quick fix. That solution is building the right data culture in your company. On the upside, there is an ever-growing list of tools available for working with data. But this profusion of tools can itself be a source of data overwhelm, and can present even more troublesome challenges than the mere quantity of data itself. You might need one database for your application, another for your analytical data, a tool to connect the two databases, one or more tools to transform data from raw to refined form, and other set of tools to visualize the data and produce reports. Any one of these choices might be daunting, but making them together can be downright paralyzing. Building an analytics stack involves so many different possible combinations of tools and systems that even CTOs and other high-level professionals often feel lost in making the right set of choices. In the tech world, changes come so rapidly that what seemed the obvious solution just a few months ago can quickly become passe. It can be very difficult to distinguish between hype and lasting value - which products will actually stand the test of time, and which are simply well-marketed but will be obsolete in a year or two? That’s why it’s so important, when developing your company’s data ecosystem, to see the big picture. Schedule a consultation today. ","date":"2023-04-14","objectID":"/posts/overwhelm/:0:0","tags":null,"title":"Overwhelmed by data?","uri":"/posts/overwhelm/"},{"categories":null,"content":"Introduction One of the many great resources for me on my professional journey has been the book Creating A Data-Driven Organization, by Carl Anderson. I would recommend it for anyone who’s involved in working with data, from managers and executives, to analysts, scientists, engineers and others. ","date":"2023-04-12","objectID":"/posts/data-driven-meaning/:1:0","tags":null,"title":"What does it mean to be 'data-driven'?","uri":"/posts/data-driven-meaning/"},{"categories":null,"content":"Prerequisites The first line of Anderson’s book is that “Data-drivenness is about building tools, abilities, and, most crucially, a culture that acts on data.” In explaining further what it means to be data-driven, Anderson defines a few prerequisites: 1. An organization must be collecting data. This isn’t as easy as it sounds, because of course you must be collecting the right data. Data has to be timely, accurate, clean, unbiased, and trustworthy. Getting high-quality data can be a lot of work. Raw data is often messy and requires cleaning - one oft-cited estimate states that data scientists spend roughly 80% of their time cleaning data and only 20% of their time building models, analyzing, and drawing conclusions. A small amount of high-quality data can be more actionable that a large quantity of junk data, so it’s important to know the difference. I once worked with a company that had a daily report on the number of new application users. It turned out, after some investigation, that the timestamp value that the analytics used to determine the day on which jobs were posted was often incorrectly updated by an errant application process. So although this company had an automated report in place to monitor daily new users, the results of that report were often wildly misleading because no one had taken the time to understand how the raw data was being handled. Making decisions based on inaccurate data can give a false sense of security, and be worse than not having analytics to begin with. 2. Data must be accessible and queryable. The data must be able to be joined to other enterprise data when necessary. This requires the right choice of tool - relational database, NoSQL, or maybe something else. Anderson cites a case study in which a company was spending a huge amount of time analyzing their data because it was all in Excel spreadsheet form. I have experienced this exact situation first-hand, working with an organization that started its analytics process by collecting data by hand from the web application and putting it into spreadsheets. The process was tedious, and this inefficiency drastically limited the amount of data they could analyze. For that company, an important first step was automating the warehousing of data into Redshift (the AWS analytical database service) so that it all existed in one place (instead of multiple spreadsheets) and could be queried using SQL. This was in 2022, but this particular organization was still analyzing its data in a way that was common twenty years earlier, meaning that it was failing to take advantages of the tremendous advances in analytics tools that have taken place in the last two decades. This sort of situation is surprisingly common. 3. You need people with the right skills to use the data. Having the right data, and storing it in the right way, is still not enough to get the best insights from that data. Your data analysts or scientists must have the appropriate skills to use the data that you have. In the case study discussed under #2 above, the company was limited by the fact that their single data analyst knew some SQL, but had no experience with Redshift, the database service they were using. So even once the data engineer built a solution to ingest the data into Redshift, the analytics process was still hobbled because the analyst wasn’t proficient with Redshift SQL. One of the reasons for this issue was that, although the analyst was eager to learn Redshift, for many months she had spent all of her time compiling data into spreadsheets and performing analysis by hand, meaning that she had no time left to learn the more important skill of querying the data from Redshift. The above points all sound simple, but the devil is in the details. Many if not most organizations struggle with relatively basic technical and organizational limitations such as those described in the above case studies. And these point describe merely the first step in taking advantage of your data -","date":"2023-04-12","objectID":"/posts/data-driven-meaning/:2:0","tags":null,"title":"What does it mean to be 'data-driven'?","uri":"/posts/data-driven-meaning/"},{"categories":null,"content":"Hallmarks Anderson goes on to describe several hallmarks of data-driven organizations to help assess the state of an organization’s analytics efforts: Data-driven organizations usually perform frequent testing, such as A/B testing to measure the relative effectiveness of different email subject lines in a marketing campaign. Such organizations must not merely be backward-looking. Most common types of analytics provide reporting and alerts about past data. But effective decision-making is concerned not only with the past, but what will likely happen in the future. Data-driven organizations are often involved in predictive modeling that provides well-informed estimates of future behavior. These organizations usually have a mindset of continuous improvement. An effective analytics culture takes the approach that ongoing development is always required to maintain a high level of insight from data, rather than thinking that, once in place, a certain system or set of reports will provide all the insight needed from ongoing data operations. Data-driven organizations will provide analysis to the correct decision-makers, who will use it as critical evidence to help inform and influence strategy. The most sophisticated analysis in the world is of little use if the right managers and executives don’t have easy access to it, and know how to use it to make critical decisions. This point, once again, might sound simplistic, but it is of crucial importance, and often overlooked. Every link in the analytics and decision-making value chain must be strong, or your organization will still fail to make the best use of its data. Of course there’s much more to creating a powerful data culture. After all, Anderson and many others have written entire books on the subject. But this overview gives you some idea of the most important issues at play in developing that culture. Oftentimes it is the most basic, and easy to make, mistakes that are the most crippling to effective data-driven decision making. I’m here to help you avoid those mistakes and implement the sound foundational principles of an effective data culture. If you have questions, or to schedule a consultation, see the contact page. ","date":"2023-04-12","objectID":"/posts/data-driven-meaning/:3:0","tags":null,"title":"What does it mean to be 'data-driven'?","uri":"/posts/data-driven-meaning/"},{"categories":null,"content":" This diagram is helpful for visualizing the relationship between data engineering, data science, and the business stakeholders. Thanks to David Holm on Twitter. A subject that is sometimes the source of ambiguity and confusion is the relationship between data analytics, data science, and data engineering. Increasingly common usage of such terms such as data analytics engineering, business intelligence engineering, and others can make it even more unclear where the boundaries between these categories and roles actually lie. Analytics can be used in a couple of ways. It is often used in a very general way to refer to the entire process of working with data, from start to finish. In that general sense, analytics incorporates data science and data engineering. But analytics can also be used more specifically to refer to the process of building reports from data that has already been ingested and transformed (a process which usually requires several steps.) Thus the data analyst role is often restricted to the ‘front end’ of the process, which involves writing queries that retrieve the appropriate data in the form needed for particular reports. Analytics and data science are often used interchangably, and there is quite a bit of overlap, but it is important to understand the differences between the two terms. Understanding data engineering helps put a finer point on the general vs. specific meanings of the term analytics. Data engineering is concerned with building the data infrastructure required for effective analytics. In this sense, data engineering is necessary to analytics, but is a largely separate area of concern. Data engineers work on the ‘back end’, meaning the part of the process dealing with the systems for ingesting, storing, processing, and otherwise preparing data for consumption into analytics reports. Data engineers make choices about what tools are necessary for various steps of the process, and how to make them work together. A data engineer might assess a use case and determine that a MySQL database is needed for the application data, that an analytical database like Redshift is needed to store processed data and deliver it for reporting. The data engineer configures these tools and the connections between them, and is also responsible for building the data pipelines which ingest and process data from a raw form into a form which is easily consumable. Tools like dbt are used to perform ETL (Extract Transform Load) processes which take raw data (often from multiple disparate sources), and combine and process them through several steps to yield refined data which can more easily be queries by data analysts and scientists. In an organization with a healthy data culture, these roles are well-defined and each person works in a particular part of the overall data process. Employees in these roles will always work closely together, and sometimes there is crossover between who performs what tasks. But in a well-functioning data culture, the responsibilities of each role are clearly delineated. Ideally, for example, data engineers focus on building infrastructure that processes data to a point where it is readily queryable and consumable by reports built by analysts. One sign of an unhealthy data culture is when the boundaries between roles are murky, and there are not clear expectations about who handles what part of the process. In one case study from my own experience, the data analyst did not have the appropriate skills for the analytical database being used (Redshift). This meant that the data engineer was often asked to write queries for reports, and build the reports in Tableau (tasks which are more properly the responsibility of the analyst). The article Engineers Shouldn’t Write ETL gives some great information about where these boundaries should ideally be drawn. ","date":"2023-04-11","objectID":"/posts/data-analytics-science-engineering/:0:0","tags":null,"title":"What's the relationship between data analytics, data science, and data engineering?","uri":"/posts/data-analytics-science-engineering/"},{"categories":null,"content":" This post is slightly off topic, but as someone who always likes to peer under the hood when I look at other people’s websites, I think some of you might appreciate it. In building this website, I made the decision to leave Joomla behind and embrace Hugo, and I couldn’t be happier with the decision. For years my business website used Joomla, the open source CMS. I picked Joomla because, after using WordPress for a while, I wanted something with more features and ability to customize. Initially when I started working with Joomla, in about 2015, I was pretty happy with it. It’s powerful, supported by all hosting providers, and has a huge ecosystem of extensions, plugins, and templates (themes). But after I managed and developed a Joomla site for a while, I started to grow increasingly frustrated with it. The first problem I found with Joomla was that, despite the huge number of available templates, all Joomla websites sort of end up looking the same. There’s a lot of customizability, but a lot of the templates are built on a handful of frameworks, so there’s not a lot of genuinely distinctive templates available. Another problem for me is that Joomla is kind of creaky. What I mean is that I felt like I was often running into technical issues that weren’t trivial to troubleshoot. More than once a change in PHP version on my hosting provider caused small errors or even broke the site entirely. It’s also bloated, since it depends on the whole LAMP stack which includes PHP and MySQL. So even for a small website you have a whole database running in the background and a lot of queries even to build basic pages. That bloat creates a lot of security issues, too. I never had e-commerce or user accounts on my site, so there wasn’t much a hacker could do if they gained access, but it was always in the back of my mind that if I was running a site with more sensitive data, Joomla might be a real nightmare. Probably my biggest source of frustration related to getting templates to work properly. Some of the first themes I tried weren’t easily customizable without a lot of custom CSS, which I’m not proficient at. Eventually I found templates built on the Helix framework, and at first those seemed pretty good. Helix provides a whole graphical interface for customizing your site, so in theory you can easily specify logos, colors, layout, etc. I say ‘in theory’ because as time went on that became increasingly less true. Even in the best case, there was always at least some small set of things that I just couldn’t get to look quite right. The site was always about 90% of what I really wanted it to look like. For a while, that was fine, since the site still looked good and functioned well. But I reached a breaking point a few days ago when I was trying to set the site up again after not maintaining it for a while. When I took a full-time salaried job last year, I stopped dealing with the website because I didn’t have time and there wasn’t much point. So when I decided to start this business, I needed to resurrect it. I thought, ’no big deal, I’ll just set hosting up again, install Joomla, and restore the site from a backup.’ That process went mostly as planned (despite the always cumbersome process of installing the database and Joomla itself, and reconfiguring some things). For some reason, with that last Joomla installation, I hit a template wall. The template I had used for a while would no longer properly reflect appearance and layout changes. I couldn’t save changes to colors, fonts, and other things. A lot of the problem, I think, had to do with the big change from Joomla 3 to Joomla 4. That seemed to break a lot of existing extensions and templates that weren’t specifically updated for v4, and I read a lot of support threads of other people having similar issues. I thought I’d find a different template/framework and ended up installing T4. That was messy because the docs were unclear and even after I had several plugins installed, my chosen ","date":"2023-04-10","objectID":"/posts/joomla-to-hugo/:0:0","tags":null,"title":"Joomla to Hugo","uri":"/posts/joomla-to-hugo/"},{"categories":null,"content":"A Jupyter notebook, hosted as a GitHub gist, with some techniques for making sure you don’t accidentally run expensive queries in BigQuery. ","date":"2023-04-09","objectID":"/posts/bigquery-cost-control/:0:0","tags":null,"title":"Bigquery Cost Control","uri":"/posts/bigquery-cost-control/"},{"categories":null,"content":" Click here to book an appointment with me on Square. Initial consultations are free. Or feel free to email me at guy@nwdataconsulting.com with questions or comments. ","date":"2020-05-23","objectID":"/contact/:0:0","tags":null,"title":"Contact","uri":"/contact/"},{"categories":null,"content":"On a lighter note, here’s a selection of streaming links for your listening pleasure: KMHD (Portland Jazz Radio) Jazz24 (Seattle Jazz Radio) ","date":"2020-05-23","objectID":"/soundtrack/:0:0","tags":null,"title":"Soundtrack","uri":"/soundtrack/"},{"categories":null,"content":" My name is Guy Cutting, and I’m located in Portland, Oregon. I have a master’s degree in Systems Science from Portland State University, and am certified as a Google Cloud engineer. I’ve lived in the Pacific Northwest for twenty years, and I have a strong sense of place, so that’s why I chose the name Northwest Data Consulting for my business. I get special satisfaction from working with companies in the Portland and wider northwest area. But dealing with data means being able to work with almost anyone from almost anywhere, and I’m happy to work with organization from across the US and the world. If you feel your company could benefit from improvements in your data culture (and almost all companies can), don’t hesitate to reach out, even if you’re not in the Portland area. I began my career in tech as a web application and database developer, building applications in ColdFusion and PHP, Microsoft SQL and MySQL. After more than a decade as a developer, I became interested in the emerging field of data science, and went back to school to get my master’s degree. I studied AI, statistical and machine learning, modeling and simulation, operations research, data warehousing, and other related subjects. After getting my master’s I worked for a while on a contract and consulting basis as a data analyst and scientist. After working on a number of projects, it became clear to me that what a lot of companies really needed to support their data analytics and data science efforts was data engineering, so I shifted focus to that newly emerging area. As a part of that process I studied cloud engineering and got my Google certification. My biggest takeaway from these years of working with companies’ data is that a lot of organizations are struggling with the quest to become data-driven. Every company wants to get as much actionable insight from their data as they possibly can, but in practice doing so can be very difficult. There are so many tools available, but that very proliferation can lead to overwhelm when trying to set up a data stack for your company. I’m passionate about helping organizations navigate this process of establishing a modern data culture. My focus is on small and medium-sized businesses that may not have as many resources as enterprise organizations. It used to be that only tech companies focused heavily on their data, but now organizations of all types must embrace putting their data to best use, or they will quickly fall behind. On a more personal note, I’m a huge jazz fan. See the soundtrack page for some streaming music links to keep you uplifted while you browse this site or go about the rest of your day. I’m also a writer of poetry, short fiction, and novels. I’m working on editing some material for posting to an online platform, so I’ll post links here when I have them. As a writer, I consider effective communication to be at least as important as any of my technical skills. I hope that this site proves informative for you, even if you choose not to employ me as a consultant. ","date":"2019-10-13","objectID":"/about/:0:0","tags":null,"title":"About","uri":"/about/"}]